= 5장. 로지스틱 회귀 : 최적화 알고리즘
* 좌표평면을 가로 지르는 선: 최적선
* 이 선을 그리는 것: 회귀

== 5.1 로지스틱 회귀와 시그모이드 함수로 분류하기: 다루기 쉬운 계단함수
* 시그모이드: S자와 유사한 커브형태의 함수. 미분 가능한 0~1사이 값 반환
image:../../images/ch5.Logistic-Regression/sigmoid.jpg[]

== 5.2 가장 좋은 회귀 계수 찾기 위한 최적화 사용
* z = w0x0 + w1x1 + .. + wnxn
** z = w^t*x
** 가장 좋은 계수 w 찾기

== 5.2.1 기울기 상승
* 함수의 기울기 = y 증가량 / x 증가량

== 5.2.2 훈련 : 기울기 상승을 사용하여 가장 좋은 매개변수 찾기
* 가중치를 1로 시작
** 입력데이터 집합의 기울기 계산
** 알파*기울기로 가중치 벡터 변경
** 가중치 벡터 반환

== 5.2.4 훈련: 확률적인 기울기 상승
* 앞에선 기울기 상승은 개산된 모든 데이터 집합을 사용
** 대안: 확률 기울기 상승 -> 한번에 단 하나의 사례를 사용하여 가중치 갱신

== 5.3 예제: 배앓이 치사율
* missing value 처리: 전처리 하기 -> 규칙을 정해서 일괄 적용 (데이터를 버리지 않는 방법)
** 가중치에 영향 없도록 누락값은 0 으로 처리

== 5.4 요약
* 확률 기울기 상승: 온라인 알고리즘 중에 하나로써 모든 데이터를 한꺼번에 처리하지 않고, 새로운 데이터가 들어올 때마다 분류기를 점진저으로 갱신
** 한번에 단 하나의 사례를 사용하여 가중치를 갱신하는 방법
