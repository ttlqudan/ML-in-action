= 5장. 로지스틱 회귀 : 최적화 알고리즘
* 좌표평면을 가로 지르는 선: 최적선
* 이 선을 그리는 것: 회귀

* 계산
** 장점: 계산 비용 적고, 구현 쉬움
** 단점: underfitting (함수 하나로 나오기 때문에)
** 활용 : 수치형 값 -> 명목형 값

step function vs sigmoid function
-> 0에서 1로 바로 감, sigmoid 는 0.4 를 0으로 보지만 어느 정도 중간치라는 걸 알 수 있다

<추가 지식>
로지스틱 회귀함수 유도: http://mazdah.tistory.com/769
로지스틱 회귀의 비용함수 : http://mazdah.tistory.com/783
로지스틱 비용함수의 미분
      -  몫의 미분법 : http://j1w2k3.tistory.com/1098
      - 합성함수의 미분 : http://j1w2k3.tistory.com/1100
      -  지수함수의 미분 : d(e^x)/dx = e^x
      - 로그함수의 미분 : http://j1w2k3.tistory.com/1102
      - 시그모이드 함수의 미분 : http://taewan.kim/post/sigmoid_diff/

<추가 설명>
P(A) 내가 승리할 확률: 0 < P(A) < 1
odd rate = P(A) / 1-P(A) => 0 ~ 무한대
log(P(A) / 1-P(A)) => -무한대 ~ 무한대, 이걸 z라고 함

y : 0,1 (실제 값는 0 아니면 1)
h : 0~1 (y값 가질 확률로 표현)

h(z) = 1 / (1+e^-z)
Z = W^t * X


sigmoid 유도 -> cost 함수 -> cost 미분
이책에선 cost 에 대한 논의가 없었음

-> 2차 함수에서 양의 함수인지 음의 함수인지에 따라 기울기 방향 혹은 기울기 반대 방향으로 가야함
==> gradient descent or ascent
==> 즉, 함수의 최대값을 구하느냐, 함수의 최소값을 구하느냐의 차이

cost(y,h): 참값과 예측값으로 cost 를 구함 ...
  => 오차율은 원래 RMSE 쓰지만 함수가 구불구불 움직여서 로컬 최소값이 구해질수 있어서 다른걸 사용함

-log(y) 는 0~1 의 log 그래프, 그리고 y = 1 일 떄 코스트 함수로 사용 가능함
-> 반대로 y = 0 일때는 -log(1-h) 를 코스트 함수로 사용
==> 합쳐서 말하면, - y * log (1-h) - (1-y) * log(h)

여기서는
x1 .. xi


h 미분하면 h(1-h) 가 됨
어떻게 그럴까?
c/w = c/h * h/z * z/w
    =        h(1-h) * Xi
    = (y-h) Xi


== 5.1 로지스틱 회귀와 시그모이드 함수로 분류하기: 다루기 쉬운 계단함수
* 시그모이드: S자와 유사한 커브형태의 함수. 미분 가능한 0~1사이 값 반환
image:../../images/ch5.Logistic-Regression/sigmoid.jpg[]

== 5.2 가장 좋은 회귀 계수 찾기 위한 최적화 사용
* z = w0x0 + w1x1 + .. + wnxn
** z = w^t*x (벡터형식 표기)
** 가장 좋은 계수 w 찾기

== 5.2.1 기울기 상승
* 함수의 최대 지점을 찾기
* 함수의 기울기 = x축 기울기 방향, y축 기울기 방향

== 5.2.2 훈련 : 기울기 상승을 사용하여 가장 좋은 매개변수 찾기
* 가중치를 1로 시작
** 입력데이터 집합의 기울기 계산
** 알파*기울기로 가중치 벡터 변경
** 가중치 벡터 반환

== 5.2.4 훈련: 확률적인 기울기 상승
* 앞에선 기울기 상승은 개산된 모든 데이터 집합을 사용
** 대안: 확률 기울기 상승 -> 한번에 단 하나의 사례를 사용하여 가중치 갱신
** 온라인 학습 알고리즘 (vs 일괄(배치)처리)

== 5.3 예제: 배앓이 치사율
* missing value 처리: 전처리 하기 -> 규칙을 정해서 일괄 적용 (데이터를 버리지 않는 방법)
** 가중치에 영향 없도록 누락값은 0 으로 처리 -> sigmoid(0) = 0.5 라서 계산 시 영향을 안주게 됨

* 일반적인 처리 방법
** 평균, 무시, 특별값으로  대체, 유사 아이템의 평균값 사용, 값 예측을 위한 다른 기계학습 사용

== 5.4 요약
* 확률 기울기 상승: 온라인 알고리즘 중에 하나로써 모든 데이터를 한꺼번에 처리하지 않고, 새로운 데이터가 들어올 때마다 분류기를 점진저으로 갱신
** 한번에 단 하나의 사례를 사용하여 가중치를 갱신하는 방법
