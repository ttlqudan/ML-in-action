= 3장. 의사결정 트리
마치 스무고개처럼 동작, 분류 기술 중 가장 일반적인 영역
의사결정 영역, 단말 영역

== 3.1 트리구조
* 장점
** 계산 비용 적음
** 학습결과를 사람이 이해 쉬움,
** 누락된 값이 있어도 처리 가능. 분류와 상관없는 속성이 있어도 처리
* 단점
** overfitting 되기 쉬움
* 적용 : 수치형값, 명목형 값
* 정보 이론
** ID3 알고리즘: 정보이득이 높은 방향으로 트리를 나눠서 하위를 구성
*** 범주형에만 사용 (수치형 불가), 상위노드에 사용된 속성은 사용 안함, overfitting 문제
** 정보이득 (IG, information gain) : 데이터 분할 전과 후의 변화
*** 이걸 낮추는 방향 -> 그래야 맞추기 좋다 (같은 것으로 분류)
** 엔트로피 (entropy) : 높을 수록 그만 큼 더 혼잡함, 정보의 기대값 (확률값에 따른 나올 값)
** 섀넌 엔트로피 :
image:../../images/ch3.decision-tree/entropy.png[]
*** 정보량? 당연한건 필요없음. 잘 안일어날수록 정보가치가 높음 (희귀할수록 정보량 높음, 확률에 반비례)
*** 각각의 정보량의 평균
** ID3 그외 알고리즘 : , C4.5, CART 등
*** C4.5 :수치 취급과 overfitting 문제 해결
*** CART : classify & regression tree, 명목형 Gini impurity (지니 불순도) 사용, 수치형은 regression 사용

== 3.2 code 설명
* 핵심: ID3 를 사용하므로, 정보이득이 많은 feature를 선택하여 데이터를 나누면서 leaf를 만들어 나감

== 3.3 분류기 검사와 저장
* classify 로 내가 가진 데이터로 값 유추

== 3.4 예제: 콘택트렌즈 유형 예측하기
* 과적합 (overfitting)
** 말 그대로 과도하게 모델을 leaning 함. 따라서 현실 세계에선 해결 못함
** 과적합 문제를 줄이려면 가지치기 (prune) 을 해야함
** 해결방법: 1.features의 수를 줄이는 방법, 2.정규화(regularzation) : 모든 feature 쓰되, 값과 크기를 줄임

* underfitting
** 다 똑같은 결론을 만들 수 있는 문제

== 3.5 요약
* 데이터의 집합 분할을 위해 엔트로피를 측정
** 너무 과하게 학습시키면 overfitting이 나타남 -> 인접 노드를 합쳐서 해결 가능
