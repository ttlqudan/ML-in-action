= 2장. k-최근접 이웃 알고리즘

* 정의: 분류나 회귀에 사용되는 비모수 방식 https://ko.wikipedia.org/wiki/K-%EC%B5%9C%EA%B7%BC%EC%A0%91_%EC%9D%B4%EC%9B%83_%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98[wiki]
* 분류를 하기 위해 거리 측정 (기본은 점과 점사이의 거리 간격)

== 2.1 거리 측정을 이용하여 분류하기
* 상위 k개의 유사한 데이터들 중 다수결을 통해 새로운 데이터의 분류 결정

* 접근방법
** 준비: 구조적 데이터 형식
** 분석: 모든 방법
** 훈련: KNN 에선 안함
** 검사: 오류율 검사함
** 사용: 입력으로부터 구조가 있는 수치형값으로 출력 필요. 그 후 분류 항목 결정

* 분류 실행 (classify0 함수)
** 거리 계산
** 정렬
** inX 와 가장 가까운 k개 아이템 추출
** k 개 아이템에서 가장 많은 분류 항목 찾기
** inX 의 분류 항목 예측을 위해 가장많은 분류 항목 반환

[source,python]
----
// inX 분류 입력 벡터 , dataSet 훈련을 위한 전체 행렬, labels 분류 항목 벡터
def classify0(inX, dataSet, labels, k):
    dataSetSize = dataSet.shape[0]
    diffMat = tile(inX, (dataSetSize,1)) - dataSet
    sqDiffMat = diffMat**2
    sqDistances = sqDiffMat.sum(axis=1)
    distances = sqDistances**0.5
    sortedDistIndicies = distances.argsort()
    classCount={}
    for i in range(k):
        voteIlabel = labels[sortedDistIndicies[i]]
        classCount[voteIlabel] = classCount.get(voteIlabel,0) + 1
    sortedClassCount = sorted(classCount.iteritems(), key=operator.itemgetter(1), reverse=True)
    return sortedClassCount[0][0]
----

* output

image:../../images/ch2.knn/ex1_result.png[]

* error rate
** 얼마나 잘됬는지 판단하기 위한 기준
** 오류율 0 이면 완벽, 1이면 모두 잘못

== 2.2 ex: kNN 이용하여 데이트 사이트 만남 주선 개선하기
* 데이트 사이트 상대의 3가지 유형
** 좋아하지 않았던 사람 / 조금 좋아했던 사람 / 많이 좋아했던 사람

[source,python]
----
def file2matrix(filename):
    fr = open(filename)
    numberOfLines = len(fr.readlines())         #get the number of lines in the file
    returnMat = zeros((numberOfLines,3))        #prepare matrix to return
    classLabelVector = []                       #prepare labels return
    fr = open(filename)
    index = 0
    for line in fr.readlines():
        line = line.strip()
        listFromLine = line.split('\t')
        returnMat[index,:] = listFromLine[0:3]
        classLabelVector.append(int(listFromLine[-1]))
        index += 1
    return returnMat,classLabelVector
----

```
datingDataMat, datingLabels = kNN.file2matrix('datingTestSet.txt')
datingDataMat
datingLabels[0:20]

>>> import matplotlib
>>> import matplotlib.pyplot as plt
>>> fig = plt.figure()
>>> ax = fig.add_subplot(111)
>>> ax.scatter(datingDataMat[:,1],datingDataMat[:,2])
>>> plt.show()
```
image:../../images/ch2.knn/plt1_video_icecream.png[]

```
# datingDataMat, datingLabels = kNN.file2matrix('datingTestSet2.txt')
from numpy import array
ax.scatter(datingDataMat[:, 1], datingDataMat[:, 2], 15.0*array(datingLabels).astype(float), 15.0*array(datingLabels).astype(float))
```
image:../../images/ch2.knn/plt1_video_icecream_color.png[]

=== 2.2.3 수치형값 정규화 하기
* 일반적으로는 정규화
** 모든값을 0~1로 하기 위해 newVal = (oldVal - min) / (max - min)

[source,python]
----
def autoNorm(dataSet):
    minVals = dataSet.min(0)
    maxVals = dataSet.max(0)
    ranges = maxVals - minVals
    normDataSet = zeros(shape(dataSet))
    m = dataSet.shape[0]
    normDataSet = dataSet - tile(minVals, (m,1)) # tile 로 입력행렬과 같은 크기 행렬 생성
    normDataSet = normDataSet/tile(ranges, (m,1))   #element wise divide
    return normDataSet, ranges, minVals
----

```
normMat, ranges, minVals = kNN.autoNorm(datingDataMat)
```

image:../../images/ch2.knn/autoNorm_result.png[]

=== 2.2.4 검사
* 훈련셋과 테스트셋 나누기 (방법은 많으나 여기선 간단히 10%를 빼서 테스트)
[source,python]
----
def datingClassTest():
    hoRatio = 0.50      #hold out 10%
    datingDataMat,datingLabels = file2matrix('datingTestSet2.txt')       #load data setfrom file
    normMat, ranges, minVals = autoNorm(datingDataMat)
    m = normMat.shape[0]
    numTestVecs = int(m*hoRatio)
    errorCount = 0.0
    for i in range(numTestVecs):
        classifierResult = classify0(normMat[i,:],normMat[numTestVecs:m,:],datingLabels[numTestVecs:m],3)
        print "the classifier came back with: %d, the real answer is: %d" % (classifierResult, datingLabels[i])
        if (classifierResult != datingLabels[i]): errorCount += 1.0
    print "the total error rate is: %f" % (errorCount/float(numTestVecs))
    print errorCount
----

image:../../images/ch2.knn/datingClassTest_result.png[]

=== 2.2.5 모든사람에게 유용
[source,python]
----
def classifyPerson():
    resultList = ['not at all', 'in small doses', 'in large doses']
    percentTats = float(raw_input(\
                                  "percentage of time spent playing video games?"))
    ffMiles = float(raw_input("frequent flier miles earned per year?"))
    iceCream = float(raw_input("liters of ice cream consumed per year?"))
    datingDataMat, datingLabels = file2matrix('datingTestSet2.txt')
    normMat, ranges, minVals = autoNorm(datingDataMat)
    inArr = array([ffMiles, percentTats, iceCream, ])
    classifierResult = classify0((inArr - \
                                  minVals)/ranges, normMat, datingLabels, 3)
    print "You will probably like this person: %s" % resultList[classifierResult - 1]
----

image:../../images/ch2.knn/classifyPerson_result.png[]

== 2.3 ex: 필기체 인식 시스템
* 0~9 번호 사용, 모두 같은 크기와 색상. 이걸 인식하기

=== 2.3.1 준비 이미지 검사 벡터 변환

[source,python]
----
def img2vector(filename):
    returnVect = zeros((1,1024))
    fr = open(filename)
    for i in range(32):
        lineStr = fr.readline()
        for j in range(32):
            returnVect[0,32*i+j] = int(lineStr[j])
    return returnVect
----

```
testVector = kNN.img2vector('testDigits/0_13.txt')
testVector[0,0:31]
testVector[0,32:63]
```

=== 2.3.2 검사: 필기체 번호에 knn 적용
[source,python]
----
def handwritingClassTest():
    hwLabels = []
    trainingFileList = listdir('trainingDigits')           #load the training set
    m = len(trainingFileList)
    trainingMat = zeros((m,1024))               # m * 1024 matrix
    for i in range(m):
        fileNameStr = trainingFileList[i]
        fileStr = fileNameStr.split('.')[0]     #take off .txt
        classNumStr = int(fileStr.split('_')[0])
        hwLabels.append(classNumStr)
        trainingMat[i,:] = img2vector('trainingDigits/%s' % fileNameStr)
    testFileList = listdir('testDigits')        #iterate through the test set
    errorCount = 0.0
    mTest = len(testFileList)
    for i in range(mTest):
        fileNameStr = testFileList[i]
        fileStr = fileNameStr.split('.')[0]     #take off .txt
        classNumStr = int(fileStr.split('_')[0])
        vectorUnderTest = img2vector('testDigits/%s' % fileNameStr)
        classifierResult = classify0(vectorUnderTest, trainingMat, hwLabels, 3)
        print "the classifier came back with: %d, the real answer is: %d" % (classifierResult, classNumStr)
        if (classifierResult != classNumStr): errorCount += 1.0
    print "\nthe total number of errors is: %d" % errorCount
    print "\nthe total error rate is: %f" % (errorCount/float(mTest))
----

```
kNN.handwritingClassTest()
```

image:../../images/ch2.knn/ex2_result.png[]

** KD-tree를 이용해 계산 횟수 즐일 수 있음

== 요약
* k 인접 알고리즘은 간단한 데이터 분류에 효과적
** 데이터 커지면 사용 힘들어짐, 데이터 구조에 대한 어떠한 저옵도 주진 못함 (즉 평균이나 모범적 사례를 모름)
