= 4장. 나이브 베이스: 확률 이론으로 분류하기
* naive? 순진한 고지식한

<기본 이론 정리>

* http://www.aistudy.com/math/conditional_probability.htm[조건부확률 예제]
* https://zetawiki.com/wiki/%EB%8F%85%EB%A6%BD%EC%82%AC%EA%B1%B4,_%EC%A2%85%EC%86%8D%EC%82%AC%EA%B1%B4,_%EB%B0%B0%EB%B0%98%EC%82%AC%EA%B1%B4[독립,조건,배반 사건]
** 독립: 서로 다른 사건이 일어날 확률에 영향 주지 않는 사건들 image:../../images/ch4.naive-bayes/독립사건.png[]
** 종속: 사건 A 가 일어났을 경우와 안일어 났을 경우에 따라 사건 B가 일어날 확률이 다를때, B는 A 에 종속사건 image:../../images/ch4.naive-bayes/종속사건.png[]
** 배반: 동시에 일어날 수 없는 두 사건 image:../../images/ch4.naive-bayes/배반사건.png[]
*  https://ko.wikipedia.org/wiki/%EB%82%98%EC%9D%B4%EB%B8%8C_%EB%B2%A0%EC%9D%B4%EC%A6%88_%EB%B6%84%EB%A5%98[나이브베이즈분류-wiki]

* 나이브 베이즈 분류를 구현시 발생하는 문제점을 해결하기 위하여 라플라스 스무딩과 로그확률을 사용
* http://untitledtblog.tistory.com/31 [머신러닝-라플라스스무딩과 로그확률에 대하여]
** 라플라스 스무딩(Laplace smoothing)
*** 확률이 0 으로되어서 각 계산이 모두 0이 될 수 있으므로, 모든 워드가1이 존재하는 가상의 문서생성
** 로그확률(Log-probability)
*** 확률은 1보다 작거나 같음. 많은 확률을 곱하면 0으로 수렴됨 -> 높은 차원데이터 해결하기 위한 방법으로 log 를 적용함
image:../../images/ch4.naive-bayes/log-probability.png[]

<추가 설명>
사전 확률과 우도
조건부 확률 P(A|B) : B 가 일어났을떄, A 가 일어날 확률
P(A|B)=P(A교B) / P(B)
=> P(A교B) = P(B)P(A|B) = P(A)P(B|A)
베이즈 정리.. 사전확률,,
P(A|B)=P(A)P(B|A) / P(B)
-> P(Ck|x1x2..xn)= P(Ck)P(x1,x2) / P(x , x, x 2)
-> 분모는 어차피 같기 때문에 보지도 않는다. (비교할 예정)

full , semi , naive 세가지 있음

각 class 별 확률 분포를 그린 후 확률 그림에 따라 면적을 구해서 class 비교도 가능

== 4.1 베이지안 의사결정 이론으로 분류
* 장점: 소량의 데이터로 작업 이뤄지며 여러개 분류 항목 다룰 수 있음
* 단점: 입력 데이터를 어떻게 준비하느냐에 따라 민감하게 작용

== 4.2 조건부 확률

== 4.3 조건부 확률로 분류하기

== 4.4 나이브 베이스로 문서 분류하기
* 독립 사건을 이용하여 단어가 많음에도 (속성이 많아도) 계산이 제곱으로 되지 않고 속성만큼 곱셈만큼만 증가함
* 단어끼리 모두 독립적 + 중요도 같음

== 4.5 파이썬으로 텍스트 분류하기

== 4.6 예제: 스팸 이메일 분류하기

== 4.7 예제: 개인 광고에 포함된 지역 특색 도출하기

== 요약
데이터 속성간에 관계가 조건부 독립으로 가정함.
즉, 문서에서 한 단어가 나올 확률은 어떤 다른 단어에 의존하지 않음 (+ 모든 속성의 중요도가 같음)
=> 잘못된 가정에도 아주 효과적
